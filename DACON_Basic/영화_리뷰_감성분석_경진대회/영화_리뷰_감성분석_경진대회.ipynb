{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈 임포트\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#!pip install konlpy\n",
    "#!pip install lightgbm\n",
    "#!pip install catboost\n",
    "#!pip install py-hanspell\n",
    "from hanspell import spell_checker\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집\n",
    "df = pd.read_csv('./train.csv')\n",
    "df_t = pd.read_csv('./test.csv')\n",
    "sub = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값이 없으므로 결측치 처리는 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 길이 확인\n",
    "# 학습용 데이터와 제출용 테스트 데이터의 전처리 및 벡터화 과정을 동시에 진행하기 위해 병합 후\n",
    "# 모델 학습 전 확인한 길이를 이용해 분리 예정\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 분포 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2564\n",
       "1    2436\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts() #1과 0의 비율이 비슷함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 (원활한 전처리를 위해 df와 df_t의 document 열을 합친 df_m 생성, cleansing 및 맞춤법 정리 후 다시 분리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m = df['document']\n",
    "df_m = df_m.append(df_t.document, ignore_index=True)\n",
    "len(df_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식을 이용한 cleansing\n",
    "df_m = df_m.apply(lambda x:re.sub('\\d+',\" \",x)) #숫자를 공백으로\n",
    "df_m = df_m.apply(lambda x:re.sub('[^a-zA-Z가-힣_]+',\" \",x)) #영어와 한글이 아닌 문자 및 자음만 있는 문자를 공백으로\n",
    "df_m = df_m.apply(lambda x:re.sub(' +',\" \",x)) #이중 공백을 공백으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춤법 정리 by hanspell\n",
    "c = []\n",
    "for i in range(len(df_m)):\n",
    "    result = spell_checker.check(df_m[i])\n",
    "    a = result.as_dict()\n",
    "    b =[] \n",
    "    b.append(a['words'].keys())\n",
    "    c.append(b)\n",
    "documents=[]\n",
    "for i in range(len(c)):\n",
    "    d=[]\n",
    "    for j in c[i][0]:\n",
    "        d.append(j)\n",
    "        e = \" \".join(d)\n",
    "        \n",
    "    documents.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맞춤법 검사 결과를 시리즈에 저장\n",
    "df_c = pd.Series(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 제작\n",
    "# cleansing 까지 진행한 학습용 데이터(df_m이 아닌 df)를 label 값에 따라 분리 후 데이터 프레임 생성\n",
    "p = df[df['label']==1]\n",
    "n = df[df['label']==0]\n",
    "p.reset_index(inplace=True)\n",
    "n.reset_index(inplace=True)\n",
    "p = p.drop(['index'], axis=1)\n",
    "n = n.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 추출\n",
    "p_pos = [] \n",
    "p_pos_sep = [] # 단어별 빈도 데이터프레임 제작용 리스트\n",
    "n_pos = []\n",
    "n_pos_sep = []\n",
    "for sentence in p['document']: \n",
    "    # 형태소 분석을 진행하고 해당 리스트를 pos라는 변수로 받음\n",
    "    pos = okt.pos(sentence,stem=True) \n",
    "    \n",
    "    # 가져오고자 하는 품사에 해당하면 해당 형태소를 main_words 리스트에 추가\n",
    "    main_words = [word_pos[0] for word_pos in pos if word_pos[1] in (\"Noun\", \"Adverb\", \"Adjective\", \"Verb\")] \n",
    "    \n",
    "    # main_words 리스트 안의 형태소들을 띄어쓰기로 분리된 하나의 문자열로 join\n",
    "    main_words_str = \" \".join(main_words) \n",
    "    p_pos.append(main_words_str)\n",
    "    \n",
    "    # p_pos_sep의 경우 하나의 문자열 대신 ,로 분리된 여러 문자로 저장\n",
    "    p_pos_sep.append(main_words)\n",
    "p[\"pos\"] = pd.DataFrame(p_pos)\n",
    "\n",
    "# 부정 리뷰에 대해서도 같은 작업 진행\n",
    "for sentence in n['document']: \n",
    "    pos = okt.pos(sentence,stem=True)\n",
    "    main_words = [word_pos[0] for word_pos in pos if word_pos[1] in (\"Noun\", \"Adverb\", \"Adjective\", \"Verb\")] \n",
    "    main_words_str = \" \".join(main_words)\n",
    "    n_pos_sep.append(main_words)\n",
    "    n_pos.append(main_words_str) \n",
    "n[\"pos\"] = pd.DataFrame(n_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 형태소와 빈도를 나타내는 빈도df 생성\n",
    "p_freq = pd.Series(np.concatenate([w for w in p_pos_sep])).value_counts()\n",
    "n_freq = pd.Series(np.concatenate([w for w in n_pos_sep])).value_counts()\n",
    "n_freq_df = pd.DataFrame(n_freq.index, columns = ['morphs'])\n",
    "p_freq_df = pd.DataFrame(p_freq.index, columns = ['morphs'])\n",
    "n_freq_df['freq']=n_freq.values\n",
    "p_freq_df['freq']=p_freq.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 낮은 빈도 단어 확인\n",
    "p_freq_df[p_freq_df['freq']<??] # ?? 자리에 원하는 숫자 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freq_df[n_freq_df['freq']<??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 낮은 빈도 단어 제거\n",
    "# 빈도 확인 후 해당 인덱스를 이용해 제거함\n",
    "p_freq_df = p_freq_df.iloc[:??,:] # ?? 자리에 인덱스 번호 넣기\n",
    "n_freq_df = n_freq_df.iloc[:??,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍정과 부정리스트 둘 다에 중복되는 단어들을 불용어로 선정 \n",
    "# dup_list4 => dup_words로 저장, 빈도50미만 제거 ver.\n",
    "dup_list4 = []\n",
    "for i in p_freq_df['morphs'].values:\n",
    "    if i in n_freq_df['morphs'].values:\n",
    "        dup_list4.append(i)\n",
    "for i in n_freq_df['morphs'].values:\n",
    "    if i in p_freq_df['morphs'].values:\n",
    "        dup_list4.append(i)\n",
    "dup_list4 = list(set(dup_list4)) # 집합을 이용해 중복 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup_list7 => dup_words2로 저장\n",
    "# 긍정, 부정 리스트 상위 20개 이내에서 (주관적으로) 유의미하게 겹치는 단어 수작업 선정\n",
    "dup_list7 = ['영화', '하다', '보다', '너무', '정말', '이다', '진짜','있다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 txt파일로 저장\n",
    "dup_words2 = pd.Series(dup_list7)\n",
    "dup_words2.to_csv('./dup_words2.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 생성(파일 명에 따라 3가지 ver 사전 사용)\n",
    "with open('./dup_words2.txt', encoding='utf-8') as fin:\n",
    "  stopwords = fin.read().split('\\n')\n",
    "stopwords = set(stopwords)\n",
    "# 사용한 사전 리스트\n",
    "# korean_stopwords\n",
    "# dup_words\n",
    "# dup_words2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 정리 tokenizer 함수 생성\n",
    "def make_token_without_stopwords(text):\n",
    "  okt = Okt()\n",
    "  tokens_In = okt.morphs(text, stem=True)\n",
    "  tokens_Out = [word for word in tokens_In if word not in stopwords] \n",
    "  return tokens_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 정리를 하지 않는 tokenizer 함수 생성\n",
    "def no_stopwords(text):\n",
    "  okt = Okt()\n",
    "  tokens_In = okt.morphs(text, stem=True)\n",
    "  tokens_Out = [word for word in tokens_In]\n",
    "  return tokens_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합쳤던 데이터를 다시 원래대로 분리, X,y 정의\n",
    "X = df_c[:5000]\n",
    "X_t = df_c[5000:]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 벡터화\n",
    "tfidf = TfidfVectorizer(min_df = 3, max_df = 0.9, tokenizer = no_stopwords, token_pattern = None)\n",
    "X=tfidf.fit_transform(X)\n",
    "X_t = tfidf.transform(X_t)\n",
    "# 불용어 정리시 tokenizer = make_token_without_stopwords 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터셋과 검증용 데이터셋 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9484692331009397 \n",
      " 0.8653733098177543\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델 생성\n",
    "# Logistic Reg.\n",
    "lr = LogisticRegression(C=2.5)\n",
    "lr.fit(X_train, y_train)\n",
    "train_score = lr.score(X_train, y_train)\n",
    "test_score = lr.score(X_test,y_test)\n",
    "print(train_score,'\\n',test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 별 train/ test score\n",
    "# korean_stopwords => 0.96/ 0.832\n",
    "# dup_words => 0.93/ 0.85\n",
    "# dup_words2 => 0.932/0.857\n",
    "# no stopword => 0.93/0.864 <- 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic 하이퍼 파라미터 튜닝\n",
    "params = {'C':np.arange(0,5,0.5), 'penalty':['l1','l2','none','elasticnet'], 'l1_ratio':np.arange(0,1,0.1), 'max_iter':np.arange(0,1000,100)}\n",
    "grid = GridSearchCV(lr, param_grid=params, scoring='accuracy',cv=10, verbose=True)\n",
    "grid.fit(X_train, y_train)\n",
    "best_lr = grid.best_estimator_\n",
    "best_lr.fit(X_train, y_train)\n",
    "train_score = best_lr.score(X_train, y_train)\n",
    "test_score = best_lr.score(X_test,y_test)\n",
    "print(train_score,'\\n',test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best hyperparameter 확인  -> C=2.5 결정\n",
    "best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features: 3000, Accuracy: 0.854\n",
      "max_features: 3100, Accuracy: 0.8566666666666667\n",
      "max_features: 3200, Accuracy: 0.8586666666666667\n",
      "max_features: 3300, Accuracy: 0.8573333333333333\n",
      "max_features: 3400, Accuracy: 0.858\n",
      "max_features: 3500, Accuracy: 0.8586666666666667\n",
      "max_features: 3600, Accuracy: 0.8606666666666667\n",
      "max_features: 3700, Accuracy: 0.8593333333333333\n",
      "max_features: 3800, Accuracy: 0.8633333333333333\n",
      "max_features: 3900, Accuracy: 0.8626666666666667\n",
      "max_features: 4000, Accuracy: 0.864\n",
      "max_features: 4100, Accuracy: 0.866\n",
      "max_features: 4200, Accuracy: 0.8653333333333333\n",
      "max_features: 4300, Accuracy: 0.8633333333333333\n",
      "max_features: 4400, Accuracy: 0.8646666666666667\n",
      "max_features: 4500, Accuracy: 0.8633333333333333\n",
      "max_features: 4600, Accuracy: 0.862\n",
      "max_features: 4700, Accuracy: 0.8626666666666667\n",
      "max_features: 4800, Accuracy: 0.8626666666666667\n",
      "max_features: 4900, Accuracy: 0.862\n",
      "max_features: 5000, Accuracy: 0.8633333333333333\n",
      "max_features: 5100, Accuracy: 0.8633333333333333\n",
      "max_features: 5200, Accuracy: 0.864\n",
      "max_features: 5300, Accuracy: 0.8633333333333333\n",
      "max_features: 5400, Accuracy: 0.8633333333333333\n",
      "max_features: 5500, Accuracy: 0.8653333333333333\n",
      "max_features: 5600, Accuracy: 0.8653333333333333\n",
      "max_features: 5700, Accuracy: 0.866\n",
      "max_features: 5800, Accuracy: 0.8653333333333333\n",
      "max_features: 5900, Accuracy: 0.866\n",
      "max_features: 6000, Accuracy: 0.8646666666666667\n",
      "max_features: 6100, Accuracy: 0.8626666666666667\n",
      "max_features: 6200, Accuracy: 0.8613333333333333\n",
      "max_features: 6300, Accuracy: 0.8626666666666667\n",
      "max_features: 6400, Accuracy: 0.8613333333333333\n",
      "max_features: 6500, Accuracy: 0.8626666666666667\n",
      "max_features: 6600, Accuracy: 0.8626666666666667\n",
      "max_features: 6700, Accuracy: 0.8633333333333333\n",
      "max_features: 6800, Accuracy: 0.864\n",
      "max_features: 6900, Accuracy: 0.8633333333333333\n",
      "max_features: 7000, Accuracy: 0.8633333333333333\n",
      "max_features: 7100, Accuracy: 0.864\n",
      "max_features: 7200, Accuracy: 0.864\n",
      "max_features: 7300, Accuracy: 0.8653333333333333\n",
      "max_features: 7400, Accuracy: 0.864\n",
      "max_features: 7500, Accuracy: 0.864\n",
      "max_features: 7600, Accuracy: 0.866\n",
      "max_features: 7700, Accuracy: 0.8666666666666667\n",
      "max_features: 7800, Accuracy: 0.8666666666666667\n",
      "max_features: 7900, Accuracy: 0.866\n",
      "max_features: 8000, Accuracy: 0.866\n",
      "max_features: 8100, Accuracy: 0.866\n",
      "max_features: 8200, Accuracy: 0.8653333333333333\n",
      "max_features: 8300, Accuracy: 0.866\n",
      "max_features: 8400, Accuracy: 0.868\n",
      "max_features: 8500, Accuracy: 0.8686666666666667\n",
      "max_features: 8600, Accuracy: 0.868\n",
      "max_features: 8700, Accuracy: 0.868\n",
      "max_features: 8800, Accuracy: 0.8693333333333333\n",
      "max_features: 8900, Accuracy: 0.8693333333333333\n",
      "max_features: 9000, Accuracy: 0.87\n",
      "max_features: 9100, Accuracy: 0.87\n",
      "max_features: 9200, Accuracy: 0.87\n",
      "max_features: 9300, Accuracy: 0.8713333333333333\n",
      "max_features: 9400, Accuracy: 0.8706666666666667\n",
      "max_features: 9500, Accuracy: 0.87\n",
      "max_features: 9600, Accuracy: 0.87\n",
      "max_features: 9700, Accuracy: 0.8706666666666667\n",
      "max_features: 9800, Accuracy: 0.8693333333333333\n",
      "max_features: 9900, Accuracy: 0.872\n",
      "max_features: 10000, Accuracy: 0.8726666666666667\n",
      "max_features: 10100, Accuracy: 0.8706666666666667\n",
      "max_features: 10200, Accuracy: 0.8713333333333333\n",
      "max_features: 10300, Accuracy: 0.8713333333333333\n",
      "max_features: 10400, Accuracy: 0.87\n",
      "max_features: 10500, Accuracy: 0.87\n",
      "max_features: 10600, Accuracy: 0.8706666666666667\n",
      "max_features: 10700, Accuracy: 0.8713333333333333\n",
      "max_features: 10800, Accuracy: 0.8713333333333333\n",
      "max_features: 10900, Accuracy: 0.8713333333333333\n",
      "max_features: 11000, Accuracy: 0.8706666666666667\n",
      "max_features: 11100, Accuracy: 0.8726666666666667\n",
      "max_features: 11200, Accuracy: 0.8713333333333333\n",
      "max_features: 11300, Accuracy: 0.8713333333333333\n",
      "max_features: 11400, Accuracy: 0.8706666666666667\n",
      "max_features: 11500, Accuracy: 0.872\n",
      "max_features: 11600, Accuracy: 0.8713333333333333\n",
      "max_features: 11700, Accuracy: 0.872\n",
      "max_features: 11800, Accuracy: 0.8706666666666667\n",
      "max_features: 11900, Accuracy: 0.8706666666666667\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectorizer 하이퍼파라미터 튜닝 -> max_feature = 11200 결정\n",
    "\n",
    "X = df_c[:5000]\n",
    "X_t = df_c[5000:]\n",
    "y = df['label']\n",
    "\n",
    "for max_features in range(3000, 12000, 100):\n",
    "    tfidf = TfidfVectorizer(min_df=0.0, max_df=0.9, analyzer=\"char\", sublinear_tf=True, ngram_range=(1, 3), max_features=max_features, tokenizer = no_stopwords, token_pattern = None)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    X_train = tfidf.fit_transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "    lr = LogisticRegression(C=2.5)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"max_features: {max_features}, Accuracy: {lr.score(X_test, y_test)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_range_n: 3, Accuracy: 0.8713333333333333\n",
      "ngram_range_n: 4, Accuracy: 0.8693333333333333\n",
      "ngram_range_n: 5, Accuracy: 0.8693333333333333\n",
      "ngram_range_n: 6, Accuracy: 0.8686666666666667\n",
      "ngram_range_n: 7, Accuracy: 0.8686666666666667\n",
      "ngram_range_n: 8, Accuracy: 0.8686666666666667\n",
      "ngram_range_n: 9, Accuracy: 0.8666666666666667\n",
      "ngram_range_n: 10, Accuracy: 0.87\n",
      "ngram_range_n: 11, Accuracy: 0.87\n",
      "ngram_range_n: 12, Accuracy: 0.8693333333333333\n",
      "ngram_range_n: 13, Accuracy: 0.868\n",
      "ngram_range_n: 14, Accuracy: 0.8666666666666667\n",
      "ngram_range_n: 15, Accuracy: 0.8673333333333333\n",
      "ngram_range_n: 16, Accuracy: 0.868\n",
      "ngram_range_n: 17, Accuracy: 0.8653333333333333\n",
      "ngram_range_n: 18, Accuracy: 0.866\n",
      "ngram_range_n: 19, Accuracy: 0.864\n",
      "ngram_range_n: 20, Accuracy: 0.8673333333333333\n",
      "ngram_range_n: 21, Accuracy: 0.8673333333333333\n",
      "ngram_range_n: 22, Accuracy: 0.866\n",
      "ngram_range_n: 23, Accuracy: 0.866\n",
      "ngram_range_n: 24, Accuracy: 0.866\n",
      "ngram_range_n: 25, Accuracy: 0.8673333333333333\n",
      "ngram_range_n: 26, Accuracy: 0.864\n",
      "ngram_range_n: 27, Accuracy: 0.8653333333333333\n",
      "ngram_range_n: 28, Accuracy: 0.8646666666666667\n",
      "ngram_range_n: 29, Accuracy: 0.8646666666666667\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectorizer 하이퍼파라미터 튜닝 -> ngram_range=(1,3) 결정\n",
    "\n",
    "X = df_c[:5000]\n",
    "X_t = df_c[5000:]\n",
    "y = df['label']\n",
    "\n",
    "for n in range(3,30):\n",
    "    tfidf = TfidfVectorizer(min_df=0.0, max_df=0.9, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,n), max_features=11200, tokenizer = no_stopwords, token_pattern = None)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    X_train = tfidf.fit_transform(X_train)\n",
    "    X_test = tfidf.transform(X_test)\n",
    "    lr = LogisticRegression(C=2.5)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"ngram_range_n: {n}, Accuracy: {lr.score(X_test, y_test)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 다시 벡터화 및 분석 진행\n",
    "\n",
    "X = df_c[:5000]\n",
    "X_t = df_c[5000:]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=0.0, max_df=0.9, analyzer=\"char\", sublinear_tf=True, ngram_range=(1, 3), max_features=11200, tokenizer = no_stopwords, token_pattern = None)\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_t = tfidf.transform(X_t)\n",
    "lr = LogisticRegression(C=2.5)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성\n",
    "sub['label']= y_pred\n",
    "sub.to_csv('./sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
